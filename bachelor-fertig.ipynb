{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import random\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import EarlyStopping\nfrom pytorch_lightning.loggers import TensorBoardLogger\nimport pandas as pd\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom sklearn.preprocessing import MinMaxScaler\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\nimport numpy as np\nimport torch\nfrom torch.nn.utils.rnn import pad_sequence\nimport torchmetrics\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\nfrom pytorch_lightning.callbacks import StochasticWeightAveraging\n\ntorch.manual_seed(0)\nnp.random.seed(0)\nrandom.seed(0)","metadata":{"ExecuteTime":{"end_time":"2023-08-08T06:47:01.775607415Z","start_time":"2023-08-08T06:47:01.642788527Z"},"execution":{"iopub.status.busy":"2023-08-08T18:58:07.015448Z","iopub.execute_input":"2023-08-08T18:58:07.015864Z","iopub.status.idle":"2023-08-08T18:58:22.216635Z","shell.execute_reply.started":"2023-08-08T18:58:07.015825Z","shell.execute_reply":"2023-08-08T18:58:22.215585Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"%load_ext tensorboard\n# %tensorboard --logdir logs","metadata":{"ExecuteTime":{"end_time":"2023-08-08T06:47:01.775796626Z","start_time":"2023-08-08T06:47:01.684992677Z"},"execution":{"iopub.status.busy":"2023-08-08T18:58:22.218720Z","iopub.execute_input":"2023-08-08T18:58:22.219114Z","iopub.status.idle":"2023-08-08T18:58:22.238660Z","shell.execute_reply.started":"2023-08-08T18:58:22.219079Z","shell.execute_reply":"2023-08-08T18:58:22.237628Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# tensorboard --logdir=tb_logs","metadata":{"ExecuteTime":{"end_time":"2023-08-08T06:47:01.775872046Z","start_time":"2023-08-08T06:47:01.685129257Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-08-08T18:58:22.240355Z","iopub.execute_input":"2023-08-08T18:58:22.240714Z","iopub.status.idle":"2023-08-08T18:58:22.245712Z","shell.execute_reply.started":"2023-08-08T18:58:22.240676Z","shell.execute_reply":"2023-08-08T18:58:22.244568Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#Ãœber r2 per hand berechnen schreiben falls richtig","metadata":{"ExecuteTime":{"end_time":"2023-08-08T06:47:01.775934167Z","start_time":"2023-08-08T06:47:01.685220308Z"},"execution":{"iopub.status.busy":"2023-08-08T18:58:22.248960Z","iopub.execute_input":"2023-08-08T18:58:22.249446Z","iopub.status.idle":"2023-08-08T18:58:22.256392Z","shell.execute_reply.started":"2023-08-08T18:58:22.249384Z","shell.execute_reply":"2023-08-08T18:58:22.255315Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)","metadata":{"ExecuteTime":{"end_time":"2023-08-08T06:47:01.778599816Z","start_time":"2023-08-08T06:47:01.688323787Z"},"execution":{"iopub.status.busy":"2023-08-08T18:58:22.258144Z","iopub.execute_input":"2023-08-08T18:58:22.258598Z","iopub.status.idle":"2023-08-08T18:58:22.277033Z","shell.execute_reply.started":"2023-08-08T18:58:22.258564Z","shell.execute_reply":"2023-08-08T18:58:22.276110Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"df1 = pd.read_csv('/kaggle/input/finalllllllll/Train/Data_processed_punctuality_202212.csv')\ndf1 = df1.sort_values(by=[\"journey_id\", \"current_stop_number\"])\ndf2 = pd.read_csv('/kaggle/input/finalllllllll/Train/Data_processed_punctuality_202207.csv')\ndf2 = df2.sort_values(by=[\"journey_id\", \"current_stop_number\"])\ndf = pd.concat([df1, df2]).reset_index(drop=True)","metadata":{"ExecuteTime":{"end_time":"2023-08-08T06:47:01.804919192Z","start_time":"2023-08-08T06:47:01.728959101Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-08-08T18:58:22.280158Z","iopub.execute_input":"2023-08-08T18:58:22.280434Z","iopub.status.idle":"2023-08-08T18:58:45.125057Z","shell.execute_reply.started":"2023-08-08T18:58:22.280409Z","shell.execute_reply":"2023-08-08T18:58:45.123748Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# df = pd.read_csv('/kaggle/working/all_data.csv')\n# df = df.sort_values(by=[\"journey_id\", \"current_stop_number\"])","metadata":{"ExecuteTime":{"end_time":"2023-08-08T06:47:01.805571575Z","start_time":"2023-08-08T06:47:01.794725298Z"},"execution":{"iopub.status.busy":"2023-08-08T18:58:45.126459Z","iopub.execute_input":"2023-08-08T18:58:45.128208Z","iopub.status.idle":"2023-08-08T18:58:45.132575Z","shell.execute_reply.started":"2023-08-08T18:58:45.128169Z","shell.execute_reply":"2023-08-08T18:58:45.131623Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# # Directory containing the CSV files\n# data_dir = '/kaggle/input/finalllllllll/Train'\n# # Lists to store the data from all files\n# df_list = []\n# for file_name in os.listdir(data_dir):\n#     # Only process CSV files\n#     if file_name.endswith('.csv'):\n#         # Join the directory path and the file name\n#         file_path = os.path.join(data_dir, file_name)\n\n#         # Read the data from the file\n#         df = pd.read_csv(file_path)\n\n#         # Append the data to the list\n#         df_list.append(df)\n\n# # Concatenate all dataframes\n# df = pd.concat(df_list, ignore_index=True)\n# df = df.sort_values(by=[\"journey_id\", \"current_stop_number\"])","metadata":{"ExecuteTime":{"end_time":"2023-08-08T06:47:01.837039807Z","start_time":"2023-08-08T06:47:01.796194553Z"},"execution":{"iopub.status.busy":"2023-08-08T18:58:45.134190Z","iopub.execute_input":"2023-08-08T18:58:45.134869Z","iopub.status.idle":"2023-08-08T18:58:45.144263Z","shell.execute_reply.started":"2023-08-08T18:58:45.134834Z","shell.execute_reply":"2023-08-08T18:58:45.143323Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# df.to_csv('all_data.csv',index=False)","metadata":{"ExecuteTime":{"end_time":"2023-08-08T06:47:01.837217128Z","start_time":"2023-08-08T06:47:01.836943457Z"},"execution":{"iopub.status.busy":"2023-08-08T18:58:45.147444Z","iopub.execute_input":"2023-08-08T18:58:45.147728Z","iopub.status.idle":"2023-08-08T18:58:45.158821Z","shell.execute_reply.started":"2023-08-08T18:58:45.147693Z","shell.execute_reply":"2023-08-08T18:58:45.157831Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def split_data_by_journey(df, val_size=0.1, test_size=0.2, random_seed=42):\n    \"\"\"Splits a DataFrame into training, validation, and test sets, \n    keeping all rows with the same journey ID in the same set.\n    \n    Parameters:\n    - df: The input DataFrame.\n    - val_size: The proportion of journey IDs to include in the validation split (default is 0.1).\n    - test_size: The proportion of journey IDs to include in the test split (default is 0.1).\n    - random_seed: The seed for the random number generator (default is 42).\n    \n    Returns:\n    - train_df: The training set as a DataFrame.\n    - val_df: The validation set as a DataFrame.\n    - test_df: The test set as a DataFrame.\n    \"\"\"\n\n    # Get a list of unique journey IDs\n    journey_ids = list(df['journey_id'].unique())\n\n    # Shuffle the journey IDs\n    np.random.seed(random_seed)\n    np.random.shuffle(journey_ids)\n\n    # Split the journey ids into train, val, and test sets\n    train_size = int((1 - val_size - test_size) * len(journey_ids))\n    val_size = int(val_size * len(journey_ids))\n    train_ids = journey_ids[:train_size]\n    val_ids = journey_ids[train_size:train_size+val_size]\n    test_ids = journey_ids[train_size+val_size:]\n\n    # Create the train, val, and test dataframes based on the split journey ids\n    train_df = df[df['journey_id'].isin(train_ids)]\n    val_df = df[df['journey_id'].isin(val_ids)]\n    test_df = df[df['journey_id'].isin(test_ids)]\n\n    return train_df, val_df, test_df\n\n\ndef generate_sequences(df):\n    \"\"\"\n    Generate sequences from the provided DataFrame.\n    Each sequence includes a number of consecutive stations in a train's journey. The function randomly selects a\n    starting station and includes all previous stations and the starting station as the input, and all subsequent\n    stations until the end of the journey as the target. The sequences of station data are returned as the inputs,\n    and the sequences of delays at the remaining stations are returned as the targets.\n\n    Args:\n    df (pandas.DataFrame): The DataFrame containing the data.\n\n    Returns:\n    inputs (list): A list of input sequences. Each sequence is a DataFrame containing the data for a number of\n                   consecutive stations.\n    targets (list): A list of target sequences. Each sequence is a Series containing the delays at the next\n                    stations.\n    \"\"\"\n    inputs = []\n    targets = []\n\n    # Group the DataFrame by train and date, so that each group represents a journey\n    df = df.sort_values(by=['journey_id', 'current_stop_number'])\n    grouped = df.groupby(['journey_id'])\n\n    # For each journey\n    for _, group in grouped:\n        # Skip the journey if it only contains one station\n        if len(group) < 10:\n            continue\n\n        group = group.reset_index(drop=True)  # Reset the index of the group\n\n        # Select a random starting point for the sequence\n        start = np.random.randint(3, len(group) - 5)\n\n        # Get the input sequence and the target sequence\n        input_sequence = group.iloc[:start]\n        target_sequence = group['arrival_delay_seconds'].iloc[start + 1:].reset_index(drop=True)\n\n        # Append the sequences to the lists\n        inputs.append(input_sequence)\n        targets.append(target_sequence)\n\n    return inputs, targets\n\n\ndef preprocess_data(train_df, val_df, test_df, cols_to_scale):\n    \"\"\"\n    Preprocess the data by scaling specified columns using MinMaxScaler.\n\n    Args:\n    - train_df (pd.DataFrame): Training dataframe.\n    - val_df (pd.DataFrame): Validation dataframe.\n    - test_df (pd.DataFrame): Test dataframe.\n    - cols_to_scale (list): List of column names to scale.\n\n    Returns:\n    - train_df (pd.DataFrame): Scaled training dataframe.\n    - val_df (pd.DataFrame): Scaled validation dataframe.\n    - test_df (pd.DataFrame): Scaled test dataframe.\n    - scaler_out (MinMaxScaler): Scaler object used for scaling.\n    \"\"\"\n    scaler_out = MinMaxScaler()\n\n    # Fit and transform the scaler on the training data\n    train_df.loc[:, cols_to_scale] = scaler_out.fit_transform(train_df[cols_to_scale])\n\n    # Transform the validation and test data using the same scaler\n    val_df.loc[:, cols_to_scale] = scaler_out.transform(val_df[cols_to_scale])\n    test_df.loc[:, cols_to_scale] = scaler_out.transform(test_df[cols_to_scale])\n\n    return train_df, val_df, test_df, scaler_out\n\n\ndef sequences_to_tensors_with_error_info(inputs, targets, columns_drop, num_errors=5):\n    \"\"\" \n    Convert sequences to tensors and return the errors for a few sequences.\n    \n    Args:\n    - inputs (list): List of input sequences.\n    - targets (list): List of target sequences.\n    - num_errors (int): Number of errors to print.\n    \n    Returns:\n    - tensor_inputs (list): List of tensor input sequences.\n    - tensor_targets (list): List of tensor target sequences.\n    - error_count (int): Number of sequences causing errors.\n    \"\"\"\n    tensor_inputs = []\n    tensor_targets = []\n    error_count = 0\n\n    for idx, (inp, tgt) in enumerate(zip(inputs, targets)):\n        try:\n            input_columns = [col for col in inp.columns if col not in columns_drop]\n            tensor_input = torch.tensor(inp[input_columns].values, dtype=torch.float32)\n            tensor_target = torch.tensor(tgt.values, dtype=torch.float32).unsqueeze(-1)\n\n            tensor_inputs.append(tensor_input)\n            tensor_targets.append(tensor_target)\n        except Exception as e:\n            error_count += 1\n            if error_count <= num_errors:\n                print(f\"Error with sequence at index '{idx}': {str(e)}\")\n                print(f\"Input sequence:\\n{inp}\")\n                print(f\"Target sequence:\\n{tgt}\\n{'-' * 50}\")\n\n    return tensor_inputs, tensor_targets, error_count","metadata":{"ExecuteTime":{"end_time":"2023-08-08T06:47:01.837957440Z","start_time":"2023-08-08T06:47:01.837112357Z"},"execution":{"iopub.status.busy":"2023-08-08T18:58:45.164016Z","iopub.execute_input":"2023-08-08T18:58:45.164310Z","iopub.status.idle":"2023-08-08T18:58:45.186970Z","shell.execute_reply.started":"2023-08-08T18:58:45.164280Z","shell.execute_reply":"2023-08-08T18:58:45.185845Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# df.info(verbose=True, show_counts=True)","metadata":{"ExecuteTime":{"end_time":"2023-08-08T06:47:01.881020883Z","start_time":"2023-08-08T06:47:01.837260278Z"},"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-08-08T18:58:45.190561Z","iopub.execute_input":"2023-08-08T18:58:45.190911Z","iopub.status.idle":"2023-08-08T18:58:45.197857Z","shell.execute_reply.started":"2023-08-08T18:58:45.190868Z","shell.execute_reply":"2023-08-08T18:58:45.196832Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"scale_cols = ['current_station_latitude', 'next_station_latitude', 'current_station_longitude',\n              'next_station_longitude', 'temperature', 'wind_peak_gust', 'next_station_temperature',\n              'next_station_wind_peak_gust', 'median_delay_next_station']\n# drop_cols=['journey_id','train_number','current_station_id','next_station_id','unique_journey_id','relation_direction',]\ndrop_cols = ['journey_id', 'train_number', 'unique_journey_id']\n# split data into train and test sets\ntrain_df, val_df, test_df = split_data_by_journey(df=df)\nprint(f\"Train set size: {train_df.shape[0]} rows\")\nprint(f\"Test set size: {test_df.shape[0]} rows\")\nprint(f\"Validation set size: {val_df.shape[0]} rows\")\ntrain_df, val_df, test_df, scaler = preprocess_data(train_df, val_df, test_df, scale_cols)\n\n\n# 1. Generate sequences using the provided method on the entire dataset\ntrain_inputs, train_targets = generate_sequences(train_df)\nval_inputs, val_targets = generate_sequences(val_df)\ntest_inputs, test_targets = generate_sequences(test_df)\n","metadata":{"ExecuteTime":{"end_time":"2023-08-08T06:47:02.160235625Z","start_time":"2023-08-08T06:47:01.880940013Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-08-08T18:58:45.201317Z","iopub.execute_input":"2023-08-08T18:58:45.201597Z","iopub.status.idle":"2023-08-08T18:59:23.370472Z","shell.execute_reply.started":"2023-08-08T18:58:45.201573Z","shell.execute_reply":"2023-08-08T18:59:23.369403Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Train set size: 2228732 rows\nTest set size: 639430 rows\nValidation set size: 319324 rows\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_28/1690434297.py:104: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  train_df.loc[:, cols_to_scale] = scaler_out.fit_transform(train_df[cols_to_scale])\n/tmp/ipykernel_28/1690434297.py:107: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  val_df.loc[:, cols_to_scale] = scaler_out.transform(val_df[cols_to_scale])\n/tmp/ipykernel_28/1690434297.py:108: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  test_df.loc[:, cols_to_scale] = scaler_out.transform(test_df[cols_to_scale])\n/tmp/ipykernel_28/1690434297.py:64: FutureWarning: In a future version of pandas, a length 1 tuple will be returned when iterating over a groupby with a grouper equal to a list of length 1. Don't supply a list with a single grouper to avoid this warning.\n  for _, group in grouped:\n/tmp/ipykernel_28/1690434297.py:64: FutureWarning: In a future version of pandas, a length 1 tuple will be returned when iterating over a groupby with a grouper equal to a list of length 1. Don't supply a list with a single grouper to avoid this warning.\n  for _, group in grouped:\n/tmp/ipykernel_28/1690434297.py:64: FutureWarning: In a future version of pandas, a length 1 tuple will be returned when iterating over a groupby with a grouper equal to a list of length 1. Don't supply a list with a single grouper to avoid this warning.\n  for _, group in grouped:\n","output_type":"stream"}]},{"cell_type":"code","source":"# Convert sequences to tensors for train, validation, and test datasets\ntensor_train_inputs, tensor_train_targets, num_errors_train = sequences_to_tensors_with_error_info(train_inputs, train_targets, drop_cols)\ntensor_val_inputs, tensor_val_targets, num_errors_val = sequences_to_tensors_with_error_info(val_inputs, val_targets, drop_cols)\ntensor_test_inputs, tensor_test_targets, num_errors_test = sequences_to_tensors_with_error_info(test_inputs, test_targets, drop_cols)\nlen(tensor_train_inputs), len(tensor_val_inputs), len(tensor_test_inputs)\nprint(f\"Number of sequences: {len(tensor_train_inputs)}\")\nprint(f\"Number of targets: {len(tensor_train_targets)}\")\nprint(f\"First sequence shape: {tensor_train_inputs[0].shape}\")\nprint(f\"First target shape: {tensor_train_targets[0].shape}\")\nprint(f\"First sequence type: {type(tensor_train_inputs[0])}\")\nprint(f\"First target type: {type(tensor_train_targets[0])}\")","metadata":{"ExecuteTime":{"end_time":"2023-08-08T06:47:02.783763784Z","start_time":"2023-08-08T06:47:02.163016575Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-08-08T18:59:23.371809Z","iopub.execute_input":"2023-08-08T18:59:23.372784Z","iopub.status.idle":"2023-08-08T19:00:37.007546Z","shell.execute_reply.started":"2023-08-08T18:59:23.372747Z","shell.execute_reply":"2023-08-08T19:00:37.006428Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Number of sequences: 62703\nNumber of targets: 62703\nFirst sequence shape: torch.Size([3, 31])\nFirst target shape: torch.Size([16, 1])\nFirst sequence type: <class 'torch.Tensor'>\nFirst target type: <class 'torch.Tensor'>\n","output_type":"stream"}]},{"cell_type":"code","source":"# %load_ext tensorboard\n%tensorboard --logdir tb_logs","metadata":{"ExecuteTime":{"end_time":"2023-08-08T06:47:02.790323265Z","start_time":"2023-08-08T06:47:02.784739968Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-08-08T19:00:37.009366Z","iopub.execute_input":"2023-08-08T19:00:37.009779Z","iopub.status.idle":"2023-08-08T19:00:44.538637Z","shell.execute_reply.started":"2023-08-08T19:00:37.009741Z","shell.execute_reply":"2023-08-08T19:00:44.537147Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n      <iframe id=\"tensorboard-frame-629f6fbed82c07cd\" width=\"100%\" height=\"800\" frameborder=\"0\">\n      </iframe>\n      <script>\n        (function() {\n          const frame = document.getElementById(\"tensorboard-frame-629f6fbed82c07cd\");\n          const url = new URL(\"/\", window.location);\n          const port = 6006;\n          if (port) {\n            url.port = port;\n          }\n          frame.src = url;\n        })();\n      </script>\n    "},"metadata":{}}]},{"cell_type":"code","source":"# Define the collate function\ndef custom_collate_fn(batch):\n    sequences, targets = zip(*batch)\n\n    # Sort sequences by length\n    sorted_lengths = sorted([(len(seq), idx) for idx, seq in enumerate(sequences)], reverse=True)\n    sorted_indices = [idx for _, idx in sorted_lengths]\n\n    sorted_sequences = [sequences[idx] for idx in sorted_indices]\n    sorted_targets = [targets[idx] for idx in sorted_indices]\n\n    lengths = [len(seq) for seq in sorted_sequences]\n\n    # Pad sequences\n    padded_sequences = pad_sequence(sorted_sequences, batch_first=True)\n    padded_targets = pad_sequence(sorted_targets, batch_first=True)\n\n    return padded_sequences, padded_targets, lengths, len(sequences)\n\n\nclass CustomDataset(Dataset):\n    def __init__(self, sequences, targets):\n        self.sequences = sequences\n        self.targets = targets\n\n    def __len__(self):\n        return len(self.sequences)\n\n    def __getitem__(self, index):\n        return self.sequences[index], self.targets[index]\n\n","metadata":{"ExecuteTime":{"end_time":"2023-08-08T06:47:02.827385487Z","start_time":"2023-08-08T06:47:02.790658136Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-08-08T19:00:44.540808Z","iopub.execute_input":"2023-08-08T19:00:44.541210Z","iopub.status.idle":"2023-08-08T19:00:44.566497Z","shell.execute_reply.started":"2023-08-08T19:00:44.541172Z","shell.execute_reply":"2023-08-08T19:00:44.558375Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"\n# Specify the directory and experiment name\nlogger = TensorBoardLogger(\"tb_logs\", name=\"Seq2SeqGRU\")\n\n\nclass Encoder(nn.Module):\n    def __init__(self, input_dim, hidden_dim, num_layers, dropout_p):\n        super(Encoder, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout_p)\n\n    def forward(self, x, lengths):\n        packed_seq = pack_padded_sequence(x, lengths, batch_first=True)\n        packed_output, h_n = self.gru(packed_seq)\n        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n        return output, h_n\n\n\nclass Decoder(nn.Module):\n    def __init__(self, output_dim, hidden_dim, num_layers, dropout_p):\n        super(Decoder, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.output_dim = output_dim\n        self.gru = nn.GRU(output_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout_p)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x, encoder_hidden):\n        output, hidden = self.gru(x, encoder_hidden)\n        output = self.fc(output)\n        return output, hidden\n\n\nclass Seq2SeqGRU(pl.LightningModule):\n    def __init__(self, input_dim, output_dim, hidden_dim, num_layers, dropout_p, learning_rate, teacher_forcing_ratio,\n                 log_grad_every=1):\n        super(Seq2SeqGRU, self).__init__()\n        self.encoder = Encoder(input_dim, hidden_dim, num_layers, dropout_p)\n        self.decoder = Decoder(output_dim, hidden_dim, num_layers, dropout_p)\n        self.learning_rate = learning_rate\n        self.teacher_forcing_ratio = teacher_forcing_ratio\n        self.log_grad_every = log_grad_every\n\n        self.train_r2 = torchmetrics.R2Score()\n        self.val_r2 = torchmetrics.R2Score()\n        self.test_r2 = torchmetrics.R2Score()\n        \n        self.train_mse = torchmetrics.MeanSquaredError()\n        self.val_mse = torchmetrics.MeanSquaredError()\n        self.test_mse = torchmetrics.MeanSquaredError()\n        \n        self.train_rmse = torchmetrics.MeanSquaredError(squared=False)\n        self.val_rmse = torchmetrics.MeanSquaredError(squared=False)\n        self.test_rmse = torchmetrics.MeanSquaredError(squared=False)\n\n\n\n    def calculate_metrics(self, output, target, mode=\"train\"):\n        \"\"\"\n        Calculate metrics and log them.\n        \"\"\"\n        # Ensure both output and target are reshaped to 2D tensors\n        output_2d = output.squeeze(-1)\n        target_2d = target.squeeze(-1)\n    \n        # Check for dimension mismatch between output and target\n        if output_2d.shape != target_2d.shape:\n            raise ValueError(f\"Shape mismatch: output {output_2d.shape} vs target {target_2d.shape}\")\n    \n        loss = F.l1_loss(output, target)\n        mse = getattr(self, f\"{mode}_mse\")(output_2d, target_2d)\n    \n        # Manual R^2 calculation\n        ss_res = torch.sum((target - output) ** 2)\n        ss_tot = torch.sum((target - torch.mean(target)) ** 2)\n        r2 = 1 - ss_res / ss_tot\n    \n        rmse = getattr(self, f\"{mode}_rmse\")(output_2d, target_2d)\n    \n        metrics = {\n            f'{mode}_loss': loss,\n            f'{mode}_mse': mse,\n            f'{mode}_rmse': rmse,\n            f'{mode}_r2': r2\n        }\n        return metrics\n\n    def forward(self, source, target, lengths):\n        # Encoder part\n        encoder_output, encoder_hidden = self.encoder(source, lengths)\n\n        # Decoder part\n        target_length = target.shape[1]\n        outputs = torch.zeros(target.shape).to(device)\n        decoder_input = target[:, 0, :].unsqueeze(1)\n\n        for t in range(1, target_length):\n            decoder_output, decoder_hidden = self.decoder(decoder_input, encoder_hidden)\n            outputs[:, t, :] = decoder_output.squeeze(1)\n            decoder_input = target[:, t, :].unsqueeze(\n                1) if torch.rand(1).item() < self.teacher_forcing_ratio else decoder_output\n\n        return outputs\n\n    def training_step(self, batch, batch_idx):\n        source, target, lengths, batch_size = batch\n        output = self.forward(source, target, lengths)\n        metrics = self.calculate_metrics(output, target, mode=\"train\")\n        self.log_dict(metrics, on_step=True, on_epoch=True, batch_size=batch_size)\n        return {'loss': metrics['train_loss']}\n\n\n    def validation_step(self, batch, batch_idx):\n        source, target, lengths, batch_size = batch\n        output = self.forward(source, target, lengths)\n        metrics = self.calculate_metrics(output, target, mode=\"val\")\n        self.log_dict(metrics, on_step=True, on_epoch=True, batch_size=batch_size)\n    \n        return {'val_loss': metrics['val_loss']}\n\n    def test_step(self, batch, batch_idx):\n        source, target, lengths, batch_size = batch\n        output = self.forward(source, target, lengths)\n        metrics = self.calculate_metrics(output, target, mode=\"test\")\n        self.log_dict(metrics, on_step=True, on_epoch=True, batch_size=batch_size)\n        return {'test_loss': metrics['test_loss']}\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate)\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min')\n        return {\n            'optimizer': optimizer,\n            'lr_scheduler': scheduler,\n            'monitor': 'val_loss'\n        }\n\n    def on_after_backward(self):\n        if self.current_epoch % self.log_grad_every == 0:  # Log every `log_grad_every` epochs\n            for name, param in self.named_parameters():\n                if param.grad is not None:\n                    self.logger.experiment.add_histogram(name + '_grad', param.grad, self.current_epoch)\n\n\ninput_dim = len(tensor_train_inputs[0][0])  # Feature length\noutput_dim = 1  # Predicting 'arrival_delay_seconds'\nhidden_dim = 512  # Hidden layer dimension\nnum_layers = 2  # Number of GRU layers\ndropout_p = 0.536079240699101  # Dropout probability\nbatch_size = 256  # Batch size for DataLoader\nepochs = 70  # Number of training epochs\nlearning_rate = 0.0009575376018764724  # Learning rate for optimizer\nteacher_forcing_ratio = 0.7338010785815597  # Probability of using teacher forcing during training\n\n# Create DataLoaders\n# Create your custom datasets\ntrain_dataset = CustomDataset(tensor_train_inputs, tensor_train_targets)\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True, collate_fn=custom_collate_fn)\n\nval_dataset = CustomDataset(tensor_val_inputs, tensor_val_targets)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True, collate_fn=custom_collate_fn)\n\ntest_dataset = CustomDataset(tensor_test_inputs, tensor_test_targets)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True, collate_fn=custom_collate_fn)\n\n\n# Model\nmodel = Seq2SeqGRU(input_dim, output_dim, hidden_dim, num_layers, dropout_p, learning_rate, teacher_forcing_ratio)\n# Define the StochasticWeightAveraging callback\n# swa = StochasticWeightAveraging(swa_epoch_start=0.8)\n# Early stopping callback\nearly_stop_callback = EarlyStopping(\n    monitor='val_loss',\n    patience=10,\n    verbose=False,\n    mode='min'\n)\n# Training\n# trainer = pl.Trainer(max_epochs=epochs, accelerator=\"auto\", callbacks=[early_stop_callback,StochasticWeightAveraging], log_every_n_steps=1,\n#                      logger=logger)\ntrainer = pl.Trainer(max_epochs=epochs, accelerator=\"auto\", callbacks=[early_stop_callback], log_every_n_steps=25,\n                     logger=logger)\n\n\n# trainer = pl.Trainer(max_epochs=epochs, accelerator=\"auto\", log_every_n_steps=1)\ntrainer.fit(model, train_loader, val_loader)","metadata":{"ExecuteTime":{"end_time":"2023-08-08T06:54:51.885315249Z","start_time":"2023-08-08T06:51:44.502651063Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-08-08T19:00:44.569232Z","iopub.execute_input":"2023-08-08T19:00:44.570910Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Sanity Checking: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d7f320517a14795bec09537112ab6f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c80d0d227abc42b6ae9be7c4d3c340f1"}},"metadata":{}}]},{"cell_type":"code","source":"%tensorboard --logdir tb_logs\n","metadata":{"ExecuteTime":{"end_time":"2023-08-08T06:50:39.588048982Z","start_time":"2023-08-08T06:50:39.583903450Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.test(dataloaders=test_loader, ckpt_path='best')","metadata":{"ExecuteTime":{"end_time":"2023-08-08T06:54:53.120468995Z","start_time":"2023-08-08T06:54:51.886160652Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df3 = pd.read_csv('/kaggle/input/finalllllllll/Train/Data_processed_punctuality_202212.csv')\ndf3 = df3.sort_values(by=[\"journey_id\", \"current_stop_number\"])\ndf3= df.head(100_000)\n\ntrain_df, val_df, test_df = split_data_by_journey(df=df3)\ntrain_df, val_df, test_df, scaler = preprocess_data(train_df, val_df, test_df, scale_cols)\ntrain_inputs, train_targets = generate_sequences(train_df)\ntrain_dataset = CustomDataset(tensor_train_inputs, tensor_train_targets)\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True, collate_fn=custom_collate_fn)\nval_dataset = CustomDataset(tensor_val_inputs, tensor_val_targets)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True, collate_fn=custom_collate_fn)\nimport optuna\n\ndef objective(trial):\n    # Hyperparameters to be optimized\n    hidden_dim = trial.suggest_int('hidden_dim', 64, 1024)\n    num_layers = trial.suggest_int('num_layers', 2, 6)\n    dropout_p = trial.suggest_float('dropout_p', 0.1, 0.7)\n    learning_rate = trial.suggest_float('lr', 1e-4, 1e-2, log=True)\n    teacher_forcing_ratio = trial.suggest_float('teacher_forcing_ratio', 0.3, 0.8)\n\n    # Model\n    model = Seq2SeqGRU(input_dim, output_dim, hidden_dim, num_layers, dropout_p, learning_rate, teacher_forcing_ratio)\n\n\n    trainer = (pl.Trainer\n               (max_epochs=20, \n                accelerator=\"auto\", \n                callbacks=[early_stop_callback], \n                log_every_n_steps=5,\n                logger=logger))\n\n    trainer.fit(model, train_loader, val_loader)\n\n    # Return the best validation loss achieved by the model\n    val_loss = trainer.callback_metrics[\"val_loss\"].item()\n    return val_loss\n","metadata":{"ExecuteTime":{"end_time":"2023-08-08T06:56:49.347273318Z","start_time":"2023-08-08T06:56:49.334773727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study = optuna.create_study(direction='minimize')  # we want to minimize the validation loss\nstudy.optimize(objective, n_trials=10)  # number of trials\n","metadata":{"ExecuteTime":{"end_time":"2023-08-08T07:03:49.739519365Z","start_time":"2023-08-08T06:56:52.079010766Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Number of finished trials: \", len(study.trials))\nprint(\"Best trial:\")\ntrial = study.best_trial\n\nprint(\"Value: \", trial.value)\nprint(\"Params: \")\nfor key, value in trial.params.items():\n    print(f\"    {key}: {value}\")\n","metadata":{"ExecuteTime":{"end_time":"2023-08-08T07:07:52.005160542Z","start_time":"2023-08-08T07:07:51.961527825Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model, 'model.pth')\n","metadata":{"ExecuteTime":{"start_time":"2023-08-08T06:50:41.145183843Z"},"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def objective(trial):\n    # Hyperparameters to be optimized\n    hidden_dim = trial.suggest_int('hidden_dim', 64, 1024)\n    num_layers = trial.suggest_int('num_layers', 2, 6)\n    dropout_p = trial.suggest_float('dropout_p', 0.1, 0.7)\n    learning_rate = trial.suggest_float('lr', 1e-4, 1e-2, log=True)\n    teacher_forcing_ratio = trial.suggest_float('teacher_forcing_ratio', 0.3, 0.8)\n\n    # Model\n    model = Seq2SeqGRU(input_dim, output_dim, hidden_dim, num_layers, dropout_p, learning_rate, teacher_forcing_ratio)\n\n    trainer = (pl.Trainer\n               (max_epochs=20, \n                accelerator=\"auto\", \n                callbacks=[early_stop_callback], \n                log_every_n_steps=5,\n                logger=logger))\n\n    trainer.fit(model, train_loader, val_loader)\n\n    val_loss = trainer.callback_metrics[\"val_loss\"].item()\n    return val_loss\nstudy = optuna.create_study(direction='minimize')  \nstudy.optimize(objective, n_trials=10) \nprint(\"Number of finished trials: \", len(study.trials))\nprint(\"Best trial:\")\ntrial = study.best_trial\n\nprint(\"Value: \", trial.value)\nprint(\"Params: \")\nfor key, value in trial.params.items():\n    print(f\"    {key}: {value}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Switch the model to evaluation mode\n# model.eval()\n# \n# # List to store all predictions\n# all_predictions = []\n# \n# # List to store actual targets\n# all_targets = []\n# \n# # Iterate over all sequences\n# for source, target, lengths in test_loader:\n#     # Move data to the device\n#     source, target = source.to(device), target.to(device)\n# \n#     # Generate prediction\n#     with torch.no_grad():\n#         output = model(source, target, lengths, 0)  # 0 teacher forcing ratio for evaluation\n# \n#     # Move prediction to CPU and convert to numpy array\n#     prediction = output.cpu().numpy()\n#     actual = target.cpu().numpy()\n# \n#     # Store prediction\n#     all_predictions.append(prediction)\n#     all_targets.append(actual)\n# \n# # Concatenate all predictions into a single numpy array\n# all_predictions = np.concatenate(all_predictions)\n# all_targets = np.concatenate(all_targets)\n# # Assuming df_test is your original test dataframe and \"target_column\" is the name of your target column\n# df_predictions = test_df.copy()\n# df_predictions[\"arrival_delay_seconds\"] = all_predictions.reshape(-1)  # Flatten the predictions array\n# \n# # Now df_predictions includes both actual and predicted target values\n# print(df_predictions)\n# df_predictions.to_csv('predictions.csv', index=False)\n","metadata":{"ExecuteTime":{"start_time":"2023-08-08T06:50:41.145205043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}